# -*- coding: utf-8 -*-
"""Tokenizer_Example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1byexJgm3SNmxeBAFSB-GMPEugqR3oBQQ
"""

"""This code demonstrates how to use the BERT tokenizer from the transformers library and SentencePieceBPETokenizer & ByteLevelBPETokenizer from the
tokenizers library to tokenize a sentence."""
!pip install transformers
!pip install tokenizers
!pip install torch

# Illustration of BertTokenizer from pre-trained model
from transformers import BertTokenizer
bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
input_S = "UnitedstatesofAmerica in total have 52 states with all independent Rights"
bert_tokens= bert_tokenizer.tokenize(input_S)
print(bert_tokens)
# Note that "##" means the token should be attached to the previous one

"""Both ByteLevelBPETokenizer and SentencePieceBPETokenizer are tokenizers used for subword tokenization, but they use different algorithms to learn the
vocabulary and perform tokenization.

ByteLevelBPETokenizer is a tokenizer from the Hugging Face tokenizers library that learns byte-level BPE (Byte Pair Encoding) subwords. It starts by
splitting each input text into bytes, and then learns a vocabulary of byte-level subwords using the BPE algorithm. This tokenizer is particularly useful
for languages with non-Latin scripts, where a character-level tokenizer may not work well.

On the other hand, SentencePieceBPETokenizer is a tokenizer from the SentencePiece library that learns subwords using a unigram language model. It first
tokenizes the input text into sentences, and then trains a unigram language model on the resulting sentence corpus to learn a vocabulary of subwords.
This tokenizer can  handle a wide range of languages and text types, and can learn both character-level and word-level subwords.

In terms of usage, both tokenizers are initialized and trained in a similar way."""

from google.colab import drive
drive.mount('/content/drive')

# Illustration of SentencePieceBPETokenizer by training the model
from tokenizers import SentencePieceBPETokenizer
BPE_sentence_tokenizer = SentencePieceBPETokenizer()
# Define the input sentence
input_sentence = "Modern agriculture has undergone a remarkable transformation over the past century, evolving from traditional farming methods to highly advanced practices driven by technology and innovation."


#Train on vocab from a file
BPE_sentence_tokenizer.train('/content/drive/MyDrive/LLM_Data/Modern_agriculture.txt', vocab_size=1000)
BPE_sentence_tokens = BPE_sentence_tokenizer.encode(input_sentence)
print(BPE_sentence_tokens.tokens)
# To decode with SentencePiece all tokens can just be concatenated and "▁" is replaced by a space.

# Illustration of ByteLevelBPETokenizer by training the model
from tokenizers import ByteLevelBPETokenizer
byte_level_BPE_tokenizer = ByteLevelBPETokenizer()
input_sentence = "Modern agriculture has undergone a remarkable transformation over the past century, evolving from traditional farming methods to highly advanced practices driven by technology and innovation."

#Train on vocab from a file
byte_level_BPE_tokenizer.train('/content/drive/MyDrive/LLM_Data/Modern_agriculture.txt', vocab_size=1000)
byte_level_BPE_tokens = byte_level_BPE_tokenizer.encode(input_sentence)
print(byte_level_BPE_tokens.tokens)
# The Ġ symbol is used to represent the beginning of a word, so when you see Ġis,
# it means the word starts with "is"

"""
#The Ġ symbol is used to represent the beginning of a word, so when you see Gi,
# it means the word starts with the letter "i"


In the encoded sentence, "Digitalsreeni" and "channel" are encoded as single
tokens because they appear frequently in the input sentence. When the Byte Pair
Encoding (BPE) algorithm is applied to the input sentence, it iteratively merges
the most frequent character sequences into new tokens until it reaches the desired
vocabulary size.

In this case, the BPE algorithm has learned that "Digitalsreeni" and "channel"
are frequent character sequences in the input sentence and has merged them into
single tokens. The Ġ symbol is used to represent the beginning of each word,
so when you see ĠDigitalsreeni, it means the word "Digitalsreeni" starts at that
position.

Note that the BPE algorithm does not always merge frequent character sequences
into single tokens. The decision to merge or not to merge depends on the frequency
and context of the character sequences in the input data. In some cases, it may be
more appropriate to split a word into multiple subword units rather than merging
it into a single token.

"""

#############################################################################
"""
#Train versus train_from_iterator

The train method takes a list of strings as input, where each string represents
a document or a sequence of tokens. It tokenizes each document and uses the
resulting tokens to learn the vocabulary of the tokenizer. This method is
suitable when the entire dataset can be loaded into memory at once.

The train_from_iterator method, on the other hand, takes an iterable as input,
where each element of the iterable represents a document or a sequence of tokens.
It iterates through the elements of the iterable and tokenizes each document,
without loading the entire dataset into memory at once. This method is useful
when the dataset is too large to fit into memory at once, or when the documents
are generated on-the-fly and cannot be pre-loaded into a list.

"""

import os

# Assigning path of the  folder containing documents need to train the model
data_dir = "/content/drive/MyDrive/LLM_Data/"

# Initialize tokenizer objects
BPE_sentence_tokenizer_1 = SentencePieceBPETokenizer()
BPE_sentence_tokenizer_2 = SentencePieceBPETokenizer()

# Method 1: Simple training at one go

# Construct full paths for each document
documents = [os.path.join(data_dir, filename) for filename in ["Modern_agriculture.txt", "Modern_agriculture_2.txt", "Modern_agriculture_3.txt"]]
BPE_sentence_tokenizer_1.train(documents)


#Method 2: Train the tokenizer using full paths with iterator

def document_iterator():
    for filename in ["Modern_agriculture.txt", "Modern_agriculture_2.txt", "Modern_agriculture_3.txt"]:
        yield os.path.join(data_dir, filename)
BPE_sentence_tokenizer_2.train_from_iterator(document_iterator())

#In both cases, we get the same training but the second approach is better
#if all training data cannot be fit in memory at once.

!pip install transformers[torch]  # Install transformers with torch dependencies

from tokenizers import ByteLevelBPETokenizer
from transformers import GPT2LMHeadModel, GPT2TokenizerFast, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments
import os

# Define paths (replace with your actual locations)
input_dir = "/content/drive/MyDrive/LLM_Data/"  # Directory containing your training text files
output_dir = "/content/drive/MyDrive/LLM_T/"  # Directory to save the trained tokenizer
model_path = "/content/drive/MyDrive/LLM_Model/"  # Directory to save the trained LLM model

# Create custom vocabulary from training data
tokenizer = ByteLevelBPETokenizer()
for filename in os.listdir(input_dir):
    with open(os.path.join(input_dir, filename), "r") as f:
        text = f.read()
        tokenizer.train_from_iterator([text], vocab_size=5000)
tokenizer.save_model(output_dir)

# Load GPT-2 tokenizer with the same vocabulary
gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(output_dir)
gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token

# Prepare the GPT-2 model
model = GPT2LMHeadModel.from_pretrained("gpt2", pad_token_id=gpt2_tokenizer.eos_token_id, num_labels=1)

# Tokenize the input text files
input_file = os.path.join(output_dir, "tokenized_data.txt")
with open(input_file, "w") as f:
    for filename in os.listdir(input_dir):
        with open(os.path.join(input_dir, filename), "r") as g:
            text = g.read()
            tokens = gpt2_tokenizer.encode(text, add_special_tokens=False)
            for token in tokens:
                f.write(str(token) + " ")
            f.write("\n")

# Load the tokenized data as a dataset
dataset = LineByLineTextDataset(
    tokenizer=gpt2_tokenizer,
    file_path=input_file,
    block_size=128,
)

# Define training arguments (adjust hyperparameters as needed)
training_args = TrainingArguments(
    output_dir=model_path,
    overwrite_output_dir=True,
    num_train_epochs=3,  # You can adjust the number of epochs for training
    per_device_train_batch_size=8,  # Adjust batch size based on Colab GPU memory
    save_steps=10000,
    save_total_limit=2,
    logging_steps=100,
    logging_dir=model_path,
)

# Data collator for grouping and padding sequences
data_collator = DataCollatorForLanguageModeling(
    tokenizer=gpt2_tokenizer, mlm=False,
)

# Initialize the Trainer for training the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

# Train the LLM model on the prepared data
trainer.train()

# Save the trained LLM model
model.save_pretrained(model_path)

print("Training complete! Your LLM model is saved in:", model_path)